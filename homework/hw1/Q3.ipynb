{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d24993-d162-4f4a-900c-eca2b649395b",
   "metadata": {},
   "source": [
    "# Q3\n",
    "In the previous question we observed some redundancy in the features. We would like to try some feature selection heuristic in this question. Consider the same dataset as question 2 (fat.csv), where **brozek** is the response variable and the other 17 columns are the model features. Follow this steps below.<br>\n",
    "\n",
    "– Form an extended version of the dataset, by appending two more columns. One column corresponding to $siri^2$ and one column corresponding to $\\frac{1}{density}$. Your extended dataset should now have 20 columns, where the first column is brozek and used as the response variable, 17 columns identical to the original fat.csv data set, and columns 19 and 20 with the values $siri^2$ and $\\frac{1}{density}$, respectively. density\n",
    "We will refer to this dataset as the *extended dataset*.\n",
    "\n",
    "– In a similar way as question 2, split the extended dataset into two sets. Set 1 includes the first 200 rows of the data (do not count the row associated with the feature/response names), and set 2, which includes the last 52 rows of the data. Name the first set **train** and the second set **test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6704b1a9-6278-4af2-bef5-957c371a6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25c09c65-19c6-4af6-8609-b0f53cedd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fat = pd.read_csv(\"fat.csv\")\n",
    "train,test = np.split(Fat,[int(200)])\n",
    "y = train['brozek']\n",
    "X_train_full = pd.DataFrame({'intercept': np.ones(train.shape[0]),\n",
    "                  'siri':         train['siri'],\n",
    "                  'density':      train['density'],\n",
    "                  'age':          train['age'],\n",
    "                  'weight':       train['weight'],\n",
    "                  'height':       train['height'],\n",
    "                  'adipos':       train['adipos'],\n",
    "                  'free':         train['free'],\n",
    "                  'neck':         train['neck'],\n",
    "                  'chest':        train['chest'],\n",
    "                  'abdom':        train['abdom'],\n",
    "                  'hip':          train['hip'],\n",
    "                  'thigh':        train['thigh'],\n",
    "                  'knee':         train['knee'],\n",
    "                  'ankle':        train['ankle'],\n",
    "                  'biceps':       train['biceps'],\n",
    "                  'forearm':      train['forearm'],\n",
    "                  'wrist':        train['wrist'],\n",
    "                  'siri_squared': train['siri'] ** 2,\n",
    "                  'inv_density':   1/train['density']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758939d9-86f2-4a55-a7ba-019b834be8fa",
   "metadata": {},
   "source": [
    "#### (a) Use the training data to fit a model of the following form <br>brozek = $\\beta_0$ + $\\beta_1$ siri + ... + $\\beta_{17}$ wrist + $\\beta_{18}$ $siri^2$ + $\\beta_{19}$ $\\frac{1}{density}$ <br>report the fitted parameters, the 95% confidence interval for each estimated parameter and the p-values. What is the R2 value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e84177a-2fee-4ed5-8cec-d3dcf78438c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>brozek</td>      <th>  R-squared:         </th> <td>   0.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.703e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 17 Apr 2024</td> <th>  Prob (F-statistic):</th> <td>6.09e-282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:15:52</td>     <th>  Log-Likelihood:    </th> <td>  64.717</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>  -89.43</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   180</td>      <th>  BIC:               </th> <td>  -23.47</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>    <td> -950.8057</td> <td>  580.610</td> <td>   -1.638</td> <td> 0.103</td> <td>-2096.483</td> <td>  194.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>siri</th>         <td>    0.9305</td> <td>    0.029</td> <td>   32.063</td> <td> 0.000</td> <td>    0.873</td> <td>    0.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>density</th>      <td>  436.7337</td> <td>  269.081</td> <td>    1.623</td> <td> 0.106</td> <td>  -94.225</td> <td>  967.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>          <td>   -0.0007</td> <td>    0.002</td> <td>   -0.414</td> <td> 0.680</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weight</th>       <td>    0.0162</td> <td>    0.006</td> <td>    2.809</td> <td> 0.006</td> <td>    0.005</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>height</th>       <td>   -0.0005</td> <td>    0.005</td> <td>   -0.096</td> <td> 0.924</td> <td>   -0.010</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>adipos</th>       <td>   -0.0235</td> <td>    0.016</td> <td>   -1.505</td> <td> 0.134</td> <td>   -0.054</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>free</th>         <td>   -0.0204</td> <td>    0.007</td> <td>   -2.780</td> <td> 0.006</td> <td>   -0.035</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>neck</th>         <td>   -0.0018</td> <td>    0.011</td> <td>   -0.163</td> <td> 0.870</td> <td>   -0.024</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chest</th>        <td>    0.0056</td> <td>    0.005</td> <td>    1.026</td> <td> 0.306</td> <td>   -0.005</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abdom</th>        <td>   -0.0006</td> <td>    0.005</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.011</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hip</th>          <td>    0.0008</td> <td>    0.008</td> <td>    0.106</td> <td> 0.916</td> <td>   -0.014</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>thigh</th>        <td>    0.0174</td> <td>    0.008</td> <td>    2.264</td> <td> 0.025</td> <td>    0.002</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>knee</th>         <td>   -0.0290</td> <td>    0.013</td> <td>   -2.296</td> <td> 0.023</td> <td>   -0.054</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ankle</th>        <td>    0.0061</td> <td>    0.010</td> <td>    0.591</td> <td> 0.555</td> <td>   -0.014</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>biceps</th>       <td>   -0.0169</td> <td>    0.008</td> <td>   -2.011</td> <td> 0.046</td> <td>   -0.034</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>forearm</th>      <td>    0.0219</td> <td>    0.010</td> <td>    2.110</td> <td> 0.036</td> <td>    0.001</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wrist</th>        <td>    0.0343</td> <td>    0.027</td> <td>    1.270</td> <td> 0.206</td> <td>   -0.019</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>siri_squared</th> <td>   -0.0026</td> <td>    0.001</td> <td>   -1.956</td> <td> 0.052</td> <td>   -0.005</td> <td> 2.26e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>inv_density</th>  <td>  518.6225</td> <td>  312.993</td> <td>    1.657</td> <td> 0.099</td> <td>  -98.985</td> <td> 1136.230</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>98.592</td> <th>  Durbin-Watson:     </th> <td>   1.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>6766.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.902</td> <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>31.437</td> <th>  Cond. No.          </th> <td>3.19e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.19e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      brozek      & \\textbf{  R-squared:         } &     0.999   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.999   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.703e+04   \\\\\n",
       "\\textbf{Date:}             & Wed, 17 Apr 2024 & \\textbf{  Prob (F-statistic):} & 6.09e-282   \\\\\n",
       "\\textbf{Time:}             &     17:15:52     & \\textbf{  Log-Likelihood:    } &    64.717   \\\\\n",
       "\\textbf{No. Observations:} &         200      & \\textbf{  AIC:               } &    -89.43   \\\\\n",
       "\\textbf{Df Residuals:}     &         180      & \\textbf{  BIC:               } &    -23.47   \\\\\n",
       "\\textbf{Df Model:}         &          19      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{intercept}     &    -950.8057  &      580.610     &    -1.638  &         0.103        &    -2096.483    &      194.871     \\\\\n",
       "\\textbf{siri}          &       0.9305  &        0.029     &    32.063  &         0.000        &        0.873    &        0.988     \\\\\n",
       "\\textbf{density}       &     436.7337  &      269.081     &     1.623  &         0.106        &      -94.225    &      967.693     \\\\\n",
       "\\textbf{age}           &      -0.0007  &        0.002     &    -0.414  &         0.680        &       -0.004    &        0.003     \\\\\n",
       "\\textbf{weight}        &       0.0162  &        0.006     &     2.809  &         0.006        &        0.005    &        0.028     \\\\\n",
       "\\textbf{height}        &      -0.0005  &        0.005     &    -0.096  &         0.924        &       -0.010    &        0.009     \\\\\n",
       "\\textbf{adipos}        &      -0.0235  &        0.016     &    -1.505  &         0.134        &       -0.054    &        0.007     \\\\\n",
       "\\textbf{free}          &      -0.0204  &        0.007     &    -2.780  &         0.006        &       -0.035    &       -0.006     \\\\\n",
       "\\textbf{neck}          &      -0.0018  &        0.011     &    -0.163  &         0.870        &       -0.024    &        0.020     \\\\\n",
       "\\textbf{chest}         &       0.0056  &        0.005     &     1.026  &         0.306        &       -0.005    &        0.016     \\\\\n",
       "\\textbf{abdom}         &      -0.0006  &        0.005     &    -0.109  &         0.913        &       -0.011    &        0.010     \\\\\n",
       "\\textbf{hip}           &       0.0008  &        0.008     &     0.106  &         0.916        &       -0.014    &        0.016     \\\\\n",
       "\\textbf{thigh}         &       0.0174  &        0.008     &     2.264  &         0.025        &        0.002    &        0.033     \\\\\n",
       "\\textbf{knee}          &      -0.0290  &        0.013     &    -2.296  &         0.023        &       -0.054    &       -0.004     \\\\\n",
       "\\textbf{ankle}         &       0.0061  &        0.010     &     0.591  &         0.555        &       -0.014    &        0.026     \\\\\n",
       "\\textbf{biceps}        &      -0.0169  &        0.008     &    -2.011  &         0.046        &       -0.034    &       -0.000     \\\\\n",
       "\\textbf{forearm}       &       0.0219  &        0.010     &     2.110  &         0.036        &        0.001    &        0.042     \\\\\n",
       "\\textbf{wrist}         &       0.0343  &        0.027     &     1.270  &         0.206        &       -0.019    &        0.088     \\\\\n",
       "\\textbf{siri\\_squared} &      -0.0026  &        0.001     &    -1.956  &         0.052        &       -0.005    &     2.26e-05     \\\\\n",
       "\\textbf{inv\\_density}  &     518.6225  &      312.993     &     1.657  &         0.099        &      -98.985    &     1136.230     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 98.592 & \\textbf{  Durbin-Watson:     } &    1.907  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } & 6766.061  \\\\\n",
       "\\textbf{Skew:}          &  0.902 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      & 31.437 & \\textbf{  Cond. No.          } & 3.19e+07  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 3.19e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 brozek   R-squared:                       0.999\n",
       "Model:                            OLS   Adj. R-squared:                  0.999\n",
       "Method:                 Least Squares   F-statistic:                 1.703e+04\n",
       "Date:                Wed, 17 Apr 2024   Prob (F-statistic):          6.09e-282\n",
       "Time:                        17:15:52   Log-Likelihood:                 64.717\n",
       "No. Observations:                 200   AIC:                            -89.43\n",
       "Df Residuals:                     180   BIC:                            -23.47\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "intercept     -950.8057    580.610     -1.638      0.103   -2096.483     194.871\n",
       "siri             0.9305      0.029     32.063      0.000       0.873       0.988\n",
       "density        436.7337    269.081      1.623      0.106     -94.225     967.693\n",
       "age             -0.0007      0.002     -0.414      0.680      -0.004       0.003\n",
       "weight           0.0162      0.006      2.809      0.006       0.005       0.028\n",
       "height          -0.0005      0.005     -0.096      0.924      -0.010       0.009\n",
       "adipos          -0.0235      0.016     -1.505      0.134      -0.054       0.007\n",
       "free            -0.0204      0.007     -2.780      0.006      -0.035      -0.006\n",
       "neck            -0.0018      0.011     -0.163      0.870      -0.024       0.020\n",
       "chest            0.0056      0.005      1.026      0.306      -0.005       0.016\n",
       "abdom           -0.0006      0.005     -0.109      0.913      -0.011       0.010\n",
       "hip              0.0008      0.008      0.106      0.916      -0.014       0.016\n",
       "thigh            0.0174      0.008      2.264      0.025       0.002       0.033\n",
       "knee            -0.0290      0.013     -2.296      0.023      -0.054      -0.004\n",
       "ankle            0.0061      0.010      0.591      0.555      -0.014       0.026\n",
       "biceps          -0.0169      0.008     -2.011      0.046      -0.034      -0.000\n",
       "forearm          0.0219      0.010      2.110      0.036       0.001       0.042\n",
       "wrist            0.0343      0.027      1.270      0.206      -0.019       0.088\n",
       "siri_squared    -0.0026      0.001     -1.956      0.052      -0.005    2.26e-05\n",
       "inv_density    518.6225    312.993      1.657      0.099     -98.985    1136.230\n",
       "==============================================================================\n",
       "Omnibus:                       98.592   Durbin-Watson:                   1.907\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6766.061\n",
       "Skew:                           0.902   Prob(JB):                         0.00\n",
       "Kurtosis:                      31.437   Cond. No.                     3.19e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.19e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullFittedModel = sm.OLS(y, X_train_full).fit()\n",
    "fullFittedModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b57a2d-3038-4b03-8d45-7484053c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full = pd.DataFrame({'intercept': np.ones(test.shape[0]),\n",
    "                  'siri':    test['siri'],\n",
    "                  'density': test['density'],\n",
    "                  'age':     test['age'],\n",
    "                  'weight':  test['weight'],\n",
    "                  'height':  test['height'],\n",
    "                  'adipos':  test['adipos'],\n",
    "                  'free':    test['free'],\n",
    "                  'neck':    test['neck'],\n",
    "                  'chest':   test['chest'],\n",
    "                  'abdom':   test['abdom'],\n",
    "                  'hip':     test['hip'],\n",
    "                  'thigh':   test['thigh'],\n",
    "                  'knee':    test['knee'],\n",
    "                  'ankle':   test['ankle'],\n",
    "                  'biceps':  test['biceps'],\n",
    "                  'forearm': test['forearm'],\n",
    "                  'wrist':   test['wrist'],\n",
    "                  'siri_squared': test['siri'] ** 2,\n",
    "                  'inv_density':   1/test['density']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4209d36-a91f-4f0d-93a7-910b2e8b82a3",
   "metadata": {},
   "source": [
    "#### (b) Use the test data to calculate the test error (similar to the formulation in part (c) of the previous question), and call it $e_{full}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b57bd656-2df4-43c0-a7b7-e7706014f17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Error e_full = 0.8565466791992765\n"
     ]
    }
   ],
   "source": [
    "# the actual value from test dataset\n",
    "y_actual = test['brozek'].values\n",
    "# the value predicted by the full-features model\n",
    "y_pred_full = fullFittedModel.predict(X_test_full)\n",
    "\n",
    "y_actual = np.array(y_actual)\n",
    "y_pred_full = np.array(y_pred_full)\n",
    "eFull = np.sqrt(sum(np.square(y_actual - y_pred_full)))\n",
    "print('Prediction Error e_full =', eFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d730c28-b593-4a01-9b71-633a8d397c9b",
   "metadata": {},
   "source": [
    "#### (c) Let’s run a heuristic scheme to perform feature selection (the method is called backward selection and described on page 79 of your textbook, also on the slides). Start with the full model (the model containing all 19 features of the extended dataset) and drop the feature with the highest p-value (or the second largest if the largest p-value is for the intercept), then redo the modeling and drop the next feature with the highest p-value, and continue dropping until all p-values are small and you are left with a set of important features. Implement this approach and stop when all p-values are below 0.03. Which features are selected as the most important ones when your code stops?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71bb1023-e9f6-417a-89bd-034b96b0af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     intercept  siri  density  age  weight  adipos   free  neck  chest  abdom  \\\n",
      "0          1.0  12.3   1.0708   23  154.25    23.7  134.9  36.2   93.1   85.2   \n",
      "1          1.0   6.1   1.0853   22  173.25    23.4  161.3  38.5   93.6   83.0   \n",
      "2          1.0  25.3   1.0414   22  154.00    24.7  116.0  34.0   95.8   87.9   \n",
      "3          1.0  10.4   1.0751   26  184.75    24.9  164.7  37.4  101.8   86.4   \n",
      "4          1.0  28.7   1.0340   24  184.25    25.6  133.1  34.4   97.3  100.0   \n",
      "..         ...   ...      ...  ...     ...     ...    ...   ...    ...    ...   \n",
      "195        1.0  25.5   1.0411   42  180.00    27.2  135.4  38.5  101.6   96.6   \n",
      "196        1.0  22.0   1.0488   42  156.25    23.1  122.6  35.5   97.8   86.0   \n",
      "197        1.0  17.7   1.0583   42  168.00    23.1  138.4  36.5   92.0   89.7   \n",
      "198        1.0   6.6   1.0841   42  167.25    22.3  155.1  37.6   94.0   78.0   \n",
      "199        1.0  23.6   1.0462   43  170.75    26.4  132.1  37.4  103.7   89.7   \n",
      "\n",
      "       hip  thigh  knee  ankle  biceps  forearm  wrist  siri_squared  \\\n",
      "0     94.5   59.0  37.3   21.9    32.0     27.4   17.1        151.29   \n",
      "1     98.7   58.7  37.3   23.4    30.5     28.9   18.2         37.21   \n",
      "2     99.2   59.6  38.9   24.0    28.8     25.2   16.6        640.09   \n",
      "3    101.2   60.1  37.3   22.8    32.4     29.4   18.2        108.16   \n",
      "4    101.9   63.2  42.2   24.0    32.2     27.7   17.7        823.69   \n",
      "..     ...    ...   ...    ...     ...      ...    ...           ...   \n",
      "195  100.6   61.1  38.4   24.1    32.9     29.8   18.8        650.25   \n",
      "196   96.2   57.7  38.6   24.0    31.2     27.3   17.4        484.00   \n",
      "197  101.0   62.3  38.0   22.3    30.8     27.8   16.9        313.29   \n",
      "198   99.0   57.5  40.0   22.5    30.6     30.0   18.5         43.56   \n",
      "199   94.2   58.5  39.0   24.1    33.8     28.8   18.8        556.96   \n",
      "\n",
      "     inv_density  \n",
      "0       0.933881  \n",
      "1       0.921404  \n",
      "2       0.960246  \n",
      "3       0.930146  \n",
      "4       0.967118  \n",
      "..           ...  \n",
      "195     0.960523  \n",
      "196     0.953471  \n",
      "197     0.944912  \n",
      "198     0.922424  \n",
      "199     0.955840  \n",
      "\n",
      "[200 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the features that has p-values over 0.03\n",
    "pv = fullFittedModel.pvalues\n",
    "pv_problematic = pv[pv > 0.03]\n",
    "\n",
    "if not pv_problematic.empty:\n",
    "    # Find the most problematic feature\n",
    "    fea_max_pv = pv.idxmax()\n",
    "\n",
    "    # Drop the problematic feature and get new training data\n",
    "    X_train_new = X_train_full.drop(fea_max_pv, axis = 1)\n",
    "    print(X_train_new)\n",
    "\n",
    "    # Retrain the model with new data\n",
    "    newFittedModel = sm.OLS(y, X_train_new).fit()\n",
    "\n",
    "else:    \n",
    "    print(\"All p-values are greater than 0.03\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5eb4cff-5472-4070-b0b2-75647701649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept       1.032524e-01\n",
      "siri            2.491931e-76\n",
      "density         1.063273e-01\n",
      "age             6.795521e-01\n",
      "weight          5.526133e-03\n",
      "height          9.239059e-01\n",
      "adipos          1.340267e-01\n",
      "free            6.010056e-03\n",
      "neck            8.704957e-01\n",
      "chest           3.063344e-01\n",
      "abdom           9.133935e-01\n",
      "hip             9.159663e-01\n",
      "thigh           2.475505e-02\n",
      "knee            2.280906e-02\n",
      "ankle           5.551942e-01\n",
      "biceps          4.579164e-02\n",
      "forearm         3.626170e-02\n",
      "wrist           2.058561e-01\n",
      "siri_squared    5.195886e-02\n",
      "inv_density     9.926578e-02\n",
      "dtype: float64\n",
      "= = = = = = = = = = = = = = = \n",
      "intercept\n",
      "= = = = = = = = = = = = = = = \n",
      "density\n"
     ]
    }
   ],
   "source": [
    "# Extract all p-values\n",
    "pv = fullFittedModel.pvalues\n",
    "print(pv)\n",
    "print(\"= = = = = = = = = = = = = = = \")\n",
    "print(pv[pv > 0.03].index[0])\n",
    "features_high_pv = pv[pv > 0.03].index[0]\n",
    "X_train_new = X_train_full.drop(['intercept'], axis = 1)\n",
    "newFittedModel = sm.OLS(y, X_train_new).fit()\n",
    "newFittedModel.summary()\n",
    "pv = newFittedModel.pvalues\n",
    "print(\"= = = = = = = = = = = = = = = \")\n",
    "print(pv[pv > 0.03].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bc381-546b-4104-8eab-44f9d7193b2f",
   "metadata": {},
   "source": [
    "#### (d) Apply the model developed in part (c) to the test data and call the error $e_{sel}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2124e7b-f6e5-4714-a578-73b6297014f8",
   "metadata": {},
   "source": [
    "#### (e) Compare $e_{full}$ and $e_{sel}$. Does the feature selection scheme seem to reduce overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad15f0c-99b5-4cef-9f44-4e601ffc3e28",
   "metadata": {},
   "source": [
    "#### (f) Compare $e_{sel}$ with $e_3$ from part (h) of question 2. In terms of the test accuracy does your feature selection scheme seem to find the best model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "608a2296-a7ac-4776-8c6e-57238b9d3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(test['siri'], y_actual, \"bo\", label=\"y_actual\")\n",
    "# ax.plot(np.hstack((test['siri'], X_test_full['siri'])), np.hstack((y_actual, y_pred_full)), \"r+\", label=\"y_predict_full\")\n",
    "# ax.legend(loc=\"best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
