{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25618b64-9024-49e8-8a18-4f32399e9ca7",
   "metadata": {},
   "source": [
    "#### The goal of this problem is to implement your own version of logistic regression, and compare it to the output of the Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3447c327-9d47-4511-90f0-f9dbf3dfdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "     (LinearDiscriminantAnalysis as LDA,\n",
    "      QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81a1be-b431-41b2-a069-f50b84881921",
   "metadata": {},
   "source": [
    "#### (a) Load the data file *BinaryData.csv* and perform a simple logistic regression in the programming language of your choice, predicting the class y based on x. Report the values of $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3029711a-63ef-4d9d-aef8-cb62e6a3935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = = Coefficient = = = = = = = = = = =\n",
      "β0: -0.7775994625128763 , β1: 1.2088079596901309\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('BinaryData.csv')\n",
    "# print(data)\n",
    "X_train = pd.DataFrame({'intercept': np.ones(train.shape[0]), 'x': data['x']})\n",
    "# print(X_train)\n",
    "y_train = (data.y == 1)\n",
    "glm_train = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "results = glm_train.fit()\n",
    "print(\"= = = = = = = = = = = Coefficient = = = = = = = = = = =\")\n",
    "beta_0, beta_1 = results.params.intercept, results.params.x\n",
    "print(\"β0:\", beta_0, \", β1:\", beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4dac4-e882-45a1-bac0-7ecb4da8a6b0",
   "metadata": {},
   "source": [
    "#### (b) Now lets work on implementing our own version of logistic regression, and understand its basics. To start, consider the function <br><br> $f(z) = \\alpha log (1 + e^{-z}) + (1 - \\alpha) log (1 + e^z)$ , $0 ≤ \\alpha ≤ 1$,<br><br>where $\\alpha$ is a known coefficient between 0 and 1. Show that $z = log( \\alpha )$ is a stationary point (point of zero derivative of $f(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418575c-07df-4e5c-9377-dd401e9f1651",
   "metadata": {},
   "source": [
    "#### (c) Show that $f(z)$ is convex (the second derivative test might be the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fa5f3-a93d-43e7-99ea-e2ae5bf86352",
   "metadata": {},
   "source": [
    "#### (d) Now that you know $f(z)$ is convex, is $z = log( \\frac{\\alpha}{1 - \\alpha}$ ) a minimizer or a maximizer? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24800cbc-7cf9-48d0-af7b-ab1ec8582d77",
   "metadata": {},
   "source": [
    "#### (e) Plot $f(z)$ for $\\alpha$ = 0.3, and the values of z between -3 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126938ed-852e-4c99-8121-07a5bacd146d",
   "metadata": {},
   "source": [
    "#### (f) In the class we learned that sum of convex functions is convex. Furthermore, we showed that if $f(z)$ is convex, $f(\\beta_0 + \\beta_1x)$ is also convex. This is an indication that the logistic loss <br><br> $L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} y_i log (1+e^{-\\beta_0-\\beta_1x_i}) + (1-y_i)log(1+e^{\\beta_0+\\beta_1x_i})$<br><br>is convex. Now, derive an expression for<br><br>$\\frac{\\partial L}{\\partial \\alpha} = ...$, $\\frac{\\partial L}{\\partial \\beta} = ...$<br><br>Simplify the expressions in a way that the end results only involve sigmoid functions and not the log or exp functions. Expressions like $\\sum_{i=1}^{n} \\omega_i\\;sigmoid(\\omega_i')+\\omega_i''$, where $\\omega_i, \\omega_i', \\omega_i''$ are expressions in terms of the problem parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51c9a7-c8b2-4948-817e-5e62c9518a37",
   "metadata": {},
   "source": [
    "#### (g) Use the data file BinaryData.csv in part (a) and set up L(β0,β1) for the xi and yi in the dataset.<br>Write a gradient descent (GD) scheme to minimize $L(β_0, β_1)$ in Matlab or Python. For your scheme use a learning rate of $η = 0.01$, and run the GD for 500 iterations. As the initial values for $β_0$ and $\\beta_1$ you can use zero (clearly, since the problem is convex, the initialization does not matter and we will converge to the global minimizer no matter where we start). Attach all your code and results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
